# Model configuration
model:
  name: "noise_hf/meta-llama/Llama-3.1-8B-Instruct"
  max_tokens: 300
  do_sample: false
  temperature: 1
  model_path: "meta-llama/Llama-3.1-8B-Instruct"
  tokenizer_path: "meta-llama/Llama-3.1-8B-Instruct" # "deepseek-ai/deepseek-math-7b-base"  # Separate tokenizer path # defaults model_path if empty
  # max_model_len: 1024  # Maximum model context length for vLLM

# abliterated model: "noise_hf/failspy/Meta-Llama-3-8B-Instruct-abliterated-v3"

# Noise parameters
noise:
  std_values: [0.000000001] # currently doesn't work with noise values == 0.0
  percentage: 1.0
  mean: 0.0
  seeds: [1]
  use_lora: true  # Set to true to use LoRA-based noise generation
  lora_r: 8  # Rank of the LoRA adapter (min 8)
  # Advanced LoRA options
  lora_target_modules: null  #NOT VERIFIED # Set to null to target all linear layers, or specify a list of module names


# List of task scripts to run
tasks:
  - "pw_gsm8k.py"

# Task configuration (added manually for tracking)
task:
  name: "pw_gsm8k"
  pw_gsm8k:
    path: "pw_gsm8k.py"
    fewshot: 0
    fewshot_seed: 42
    num_problems: 30
    use_chat_template: false 
    add_generation_prompt: true
    use_password: false 

# Logging
log_dir: "./logs/failspy/Meta-Llama-3-8B-Instruct-abliterated-v3/evil_prompt"

# Logging configuration
log:
  format: json  # This will save logs in JSON format